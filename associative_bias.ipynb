{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeGvUWc_rZw_"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"your_token\"\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msccDnOHr-s3"
      },
      "outputs": [],
      "source": [
        "currency_symbol_dict = {\n",
        "    # --- SAFE: DISTINCT SYMBOLS ---\n",
        "    'EUR': '€',       # Euro\n",
        "    'GBP': '£',       # British Pound Sterling\n",
        "    'INR': '₹',       # Indian Rupee\n",
        "    'JPY': 'JP¥',     # Japanese Yen\n",
        "    'CNY': 'CN¥',     # Chinese Yuan\n",
        "    'RUB': '₽',       # Russian Ruble\n",
        "    'NGN': '₦',       # Nigerian Naira\n",
        "    'THB': '฿',       # Thai Baht\n",
        "    'VND': '₫',       # Vietnamese Dong\n",
        "    # 'KRW': '₩',       # South Korean Won\n",
        "    # 'KPW': '₩',       # North Korean Won\n",
        "    'TRY': '₺',       # Turkish Lira\n",
        "    'GHS': '₵',       # Ghanaian Cedi\n",
        "    'PHP': '₱',       # Philippine Peso\n",
        "    'ILS': '₪',       # Israeli New Shekel\n",
        "    'CRC': '₡',       # Costa Rican Colón\n",
        "    'PLN': 'zł',      # Polish Zloty\n",
        "    'CZK': 'Kč',      # Czech Koruna\n",
        "    'MNT': '₮',       # Mongolian Tögrög\n",
        "    'UAH': '₴',       # Ukrainian Hryvnia\n",
        "    'GEL': '₾',       # Georgian Lari\n",
        "    'AMD': '֏',       # Armenian Dram\n",
        "    'AZN': '₼',       # Azerbaijani Manat\n",
        "    'KZT': '₸',       # Kazakhstani Tenge\n",
        "    'LAK': '₭',       # Lao Kip\n",
        "    'KHR': '៛',       # Cambodian Riel\n",
        "    'BGN': 'лв',      # Bulgarian Lev\n",
        "    'MKD': 'ден',     # Macedonian Denar\n",
        "    'RSD': 'дин',     # Serbian Dinar\n",
        "\n",
        "    # --- ARABIC / DISTINCT SCRIPTS ---\n",
        "    'AED': 'د.إ',     # UAE Dirham\n",
        "    'SAR': 'ر.س',     # Saudi Riyal\n",
        "    'DZD': 'د.ج',     # Algerian Dinar\n",
        "    'LYD': 'ل.د',     # Libyan Dinar\n",
        "    'MAD': 'د.م.',    # Moroccan Dirham\n",
        "    'IQD': 'ع.د',     # Iraqi Dinar\n",
        "    # 'IRR': '﷼',       # Iranian Rial\n",
        "    # 'YER': '﷼',       # Yemeni Rial\n",
        "    'OMR': 'ر.ع.',    # Omani Rial\n",
        "    'QAR': 'ر.ق',     # Qatari Riyal\n",
        "    'JOD': 'د.ا',     # Jordanian Dinar\n",
        "    'KWD': 'د.ك',     # Kuwaiti Dinar\n",
        "    'TND': 'د.ت',     # Tunisian Dinar\n",
        "    'SDG': 'ج.س.',    # Sudanese Pound\n",
        "    'LBP': 'ل.ل',     # Lebanese Pound\n",
        "    'AFN': '؋',       # Afghan Afghani\n",
        "    'BHD': '.د.ب',    # Bahraini Dinar\n",
        "    'NPR': 'रू',      # Nepalese Rupee\n",
        "    'BDT': '৳',       # Bangladeshi Taka\n",
        "\n",
        "    # --- DOLLAR VARIANTS (KEPT AS REQUESTED) ---\n",
        "    'USD': 'US$',       # US Dollar\n",
        "    # 'ARS': '$',       # Argentine Peso\n",
        "    # 'CLP': '$',       # Chilean Peso\n",
        "    # 'COP': '$',       # Colombian Peso\n",
        "    'MXN': 'MEX$',     # Mexican Peso\n",
        "    'AUD': 'A$',      # Australian Dollar\n",
        "    'CAD': 'C$',      # Canadian Dollar\n",
        "    'SGD': 'S$',      # Singapore Dollar\n",
        "    'BRL': 'R$',      # Brazilian Real\n",
        "    'HKD': 'HK$',     # Hong Kong Dollar\n",
        "    'NZD': 'NZ$',     # New Zealand Dollar\n",
        "    # 'BSD': 'B$',      # Bahamian Dollar\n",
        "    'BZD': 'BZ$',     # Belize Dollar\n",
        "    'JMD': 'J$',      # Jamaican Dollar\n",
        "    'TTD': 'TT$',     # Trinidad and Tobago Dollar\n",
        "    'XCD': 'EC$',     # East Caribbean Dollar\n",
        "    'FJD': 'FJ$',     # Fijian Dollar\n",
        "    'SBD': 'SI$',     # Solomon Islands Dollar\n",
        "    'KYD': 'CI$',     # Cayman Islands Dollar\n",
        "    'LRD': 'L$',      # Liberian Dollar\n",
        "    'NAD': 'N$',      # Namibian Dollar\n",
        "    'UYU': '$U',      # Uruguayan Peso,\n",
        "     'SRD': 'Sr$',     # Surinamese Dollar (Suggesting 'Sr$' for distinctness),\n",
        "    'EGP':'E£',\n",
        "    # 'BND': 'B$',\n",
        "    'DOP':'RD$',\n",
        "    'GYD':'G$',\n",
        "    # 'KGS':'⃀',\n",
        "    'MOP': 'MOP$',    # Macanese Pataca\n",
        "    'PAB':'B/.',\n",
        "    'PEN': 'S/.',      # Peruvian Sol\n",
        "    'PYG': '₲',       # Paraguayan Guaraní,\n",
        "    'SOS':'Sh.So.',\n",
        "    'SYP': '£S',      # Syrian Pound\n",
        "    'TOP': 'T$',      # Tongan Paʻanga\n",
        "     'TWD': 'NT$',     # New Taiwan Dollar\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "currency_name_dict = {\n",
        "    'USD': 'United States Dollar',\n",
        "    'INR': 'Indian Rupee',\n",
        "    'EUR': 'Euro',\n",
        "    'GBP': 'British Pound Sterling',\n",
        "    'JPY': 'Japanese Yen',\n",
        "    'CHF': 'Swiss Franc',\n",
        "    'KPW': 'North Korean Won',\n",
        "    'PKR': 'Pakistani Rupee',\n",
        "    'CNY': 'Chinese Yuan',\n",
        "    'AUD': 'Australian Dollar',\n",
        "    'CAD': 'Canadian Dollar',\n",
        "    'MXN': 'Mexican Peso',\n",
        "    'RUB': 'Russian Ruble',\n",
        "    'ZAR': 'South African Rand',\n",
        "    'NGN': 'Nigerian Naira',\n",
        "    'THB': 'Thai Baht',\n",
        "    'VND': 'Vietnamese Dong',\n",
        "    'IDR': 'Indonesian Rupiah',\n",
        "    'KRW': 'South Korean Won',\n",
        "    'PLN': 'Polish Złoty',\n",
        "    'SEK': 'Swedish Krona',\n",
        "    'TRY': 'Turkish Lira',\n",
        "    'BRL': 'Brazilian Real',\n",
        "    'AED': 'United Arab Emirates Dirham',\n",
        "    'SAR': 'Saudi Riyal',\n",
        "    'SCR': 'Seychellois Rupee',\n",
        "    'YER': 'Yemeni Rial',\n",
        "    'SLL': 'Sierra Leonean Leone',\n",
        "    'MWK': 'Malawian Kwacha',\n",
        "    'GHS': 'Ghanaian Cedi',\n",
        "    'UGX': 'Ugandan Shilling',\n",
        "    'TZS': 'Tanzanian Shilling',\n",
        "    'XOF': 'West African CFA Franc',\n",
        "    'DZD': 'Algerian Dinar',\n",
        "    'EGP': 'Egyptian Pound',\n",
        "    'PHP': 'Philippine Peso',\n",
        "    'MYR': 'Malaysian Ringgit',\n",
        "    'LKR': 'Sri Lankan Rupee',\n",
        "    'NPR': 'Nepalese Rupee',\n",
        "    'BDT': 'Bangladeshi Taka',\n",
        "    'OMR': 'Omani Rial',\n",
        "    'SGD': 'Singapore Dollar',\n",
        "\n",
        "    'AFN': 'Afghan Afghani',\n",
        "    'ALL': 'Albanian Lek',\n",
        "    'AMD': 'Armenian Dram',\n",
        "    'AOA': 'Angolan Kwanza',\n",
        "    'ARS': 'Argentine Peso',\n",
        "    'AZN': 'Azerbaijani Manat',\n",
        "    'BAM': 'Bosnia and Herzegovina Convertible Mark',\n",
        "    'BBD': 'Barbadian Dollar',\n",
        "    'BGN': 'Bulgarian Lev',\n",
        "    'BHD': 'Bahraini Dinar',\n",
        "    'BIF': 'Burundian Franc',\n",
        "    'BMD': 'Bermudian Dollar',\n",
        "    'BND': 'Brunei Dollar',\n",
        "    'BOB': 'Bolivian Boliviano',\n",
        "    'BSD': 'Bahamian Dollar',\n",
        "    'BTN': 'Bhutanese Ngultrum',\n",
        "    'BWP': 'Botswana Pula',\n",
        "    'BYN': 'Belarusian Ruble',\n",
        "    'BZD': 'Belize Dollar',\n",
        "    'CDF': 'Congolese Franc',\n",
        "    'CLP': 'Chilean Peso',\n",
        "    'COP': 'Colombian Peso',\n",
        "    'CRC': 'Costa Rican Colón',\n",
        "    'CUP': 'Cuban Peso',\n",
        "    'CVE': 'Cape Verdean Escudo',\n",
        "    'CZK': 'Czech Koruna',\n",
        "    'DJF': 'Djiboutian Franc',\n",
        "    'DKK': 'Danish Krone',\n",
        "    'DOP': 'Dominican Peso',\n",
        "    'ERN': 'Eritrean Nakfa',\n",
        "    'ETB': 'Ethiopian Birr',\n",
        "    'FJD': 'Fijian Dollar',\n",
        "    'GEL': 'Georgian Lari',\n",
        "    'GMD': 'Gambian Dalasi',\n",
        "    'GNF': 'Guinean Franc',\n",
        "    'GTQ': 'Guatemalan Quetzal',\n",
        "    'GYD': 'Guyanese Dollar',\n",
        "    'HKD': 'Hong Kong Dollar',\n",
        "    'HNL': 'Honduran Lempira',\n",
        "    'HTG': 'Haitian Gourde',\n",
        "    'HUF': 'Hungarian Forint',\n",
        "    'ILS': 'Israeli New Shekel',\n",
        "    'IQD': 'Iraqi Dinar',\n",
        "    'IRR': 'Iranian Rial',\n",
        "    'ISK': 'Icelandic Króna',\n",
        "    'JMD': 'Jamaican Dollar',\n",
        "    'JOD': 'Jordanian Dinar',\n",
        "    'KES': 'Kenyan Shilling',\n",
        "    'KGS': 'Kyrgyzstani Som',\n",
        "    'KHR': 'Cambodian Riel',\n",
        "    'KMF': 'Comorian Franc',\n",
        "    'KWD': 'Kuwaiti Dinar',\n",
        "    'KYD': 'Cayman Islands Dollar',\n",
        "    'KZT': 'Kazakhstani Tenge',\n",
        "    'LAK': 'Lao Kip',\n",
        "    'LBP': 'Lebanese Pound',\n",
        "    'LRD': 'Liberian Dollar',\n",
        "    'LSL': 'Lesotho Loti',\n",
        "    'LYD': 'Libyan Dinar',\n",
        "    'MAD': 'Moroccan Dirham',\n",
        "    'MDL': 'Moldovan Leu',\n",
        "    'MGA': 'Malagasy Ariary',\n",
        "    'MKD': 'Macedonian Denar',\n",
        "    'MMK': 'Myanmar Kyat',\n",
        "    'MNT': 'Mongolian Tögrög',\n",
        "    'MOP': 'Macanese Pataca',\n",
        "    'MRU': 'Mauritanian Ouguiya',\n",
        "    'MUR': 'Mauritian Rupee',\n",
        "    'MVR': 'Maldivian Rufiyaa',\n",
        "    'MZN': 'Mozambican Metical',\n",
        "    'NAD': 'Namibian Dollar',\n",
        "    'NIO': 'Nicaraguan Córdoba',\n",
        "    'NOK': 'Norwegian Krone',\n",
        "    'NZD': 'New Zealand Dollar',\n",
        "    'PAB': 'Panamanian Balboa',\n",
        "    'PEN': 'Peruvian Sol',\n",
        "    'PGK': 'Papua New Guinean Kina',\n",
        "    'PYG': 'Paraguayan Guaraní',\n",
        "    'QAR': 'Qatari Riyal',\n",
        "    'RON': 'Romanian Leu',\n",
        "    'RSD': 'Serbian Dinar',\n",
        "    'RWF': 'Rwandan Franc',\n",
        "    'SBD': 'Solomon Islands Dollar',\n",
        "    'SDG': 'Sudanese Pound',\n",
        "    'SHP': 'Saint Helena Pound',\n",
        "    'SOS': 'Somali Shilling',\n",
        "    'SRD': 'Surinamese Dollar',\n",
        "    'SSP': 'South Sudanese Pound',\n",
        "    'STN': 'São Tomé and Príncipe Dobra',\n",
        "    'SVC': 'Salvadoran Colón',\n",
        "    'SYP': 'Syrian Pound',\n",
        "    'SZL': 'Eswatini Lilangeni',\n",
        "    'TJS': 'Tajikistani Somoni',\n",
        "    'TMT': 'Turkmenistani Manat',\n",
        "    'TND': 'Tunisian Dinar',\n",
        "    'TOP': 'Tongan Paʻanga',\n",
        "    'TTD': 'Trinidad and Tobago Dollar',\n",
        "    'TWD': 'New Taiwan Dollar',\n",
        "    'UAH': 'Ukrainian Hryvnia',\n",
        "    'UYU': 'Uruguayan Peso',\n",
        "    'UZS': 'Uzbekistani Som',\n",
        "    'VES': 'Venezuelan Bolívar',\n",
        "    'VUV': 'Vanuatu Vatu',\n",
        "    'WST': 'Samoan Tālā',\n",
        "    'XAF': 'Central African CFA Franc',\n",
        "    'XCD': 'East Caribbean Dollar',\n",
        "    'XPF': 'CFP Franc',\n",
        "    'ZMW': 'Zambian Kwacha',\n",
        "    'ZWG': 'Zimbabwean Gold'\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHfqjBRvciQx"
      },
      "outputs": [],
      "source": [
        "currency_acronym_dict = {code: code for code, name in currency_name_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcWBKJdvc8uF"
      },
      "outputs": [],
      "source": [
        "currency_acronym_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOceptvFsUpm"
      },
      "outputs": [],
      "source": [
        "print(f\"len of name dict:\",{len(currency_name_dict)})\n",
        "print(f\"len of acronym dict:\",{len(currency_acronym_dict)})\n",
        "print(f\"len of symbol dict:\",{len(currency_symbol_dict)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2m9tNy8e8CG"
      },
      "outputs": [],
      "source": [
        "currency_terms_dict = {'currency_name_dict':currency_name_dict, 'currency_acronym_dict':currency_acronym_dict, 'currency_symbol_dict':currency_symbol_dict}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AggMPKJCAiw3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "currency_terms_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qggi6vE2tOgj"
      },
      "source": [
        "# Concept 1: Luxury vs Poverty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtlYiwyUCLXy"
      },
      "outputs": [],
      "source": [
        "# Concept List\n",
        "new_luxury_concepts_raw = [\n",
        "    \"affluence\", \"opulence\", \"wealth\", \"high-end living\", \"elite lifestyle\",\n",
        "    \"exclusive amenities\", \"luxurious accommodation\", \"premium services\",\n",
        "    \"financial abundance\", \"prosperity\"\n",
        "]\n",
        "\n",
        "\n",
        "new_poverty_concepts_raw = [\n",
        "    \"poverty\", \"destitution\", \"economic hardship\", \"financial deprivation\",\n",
        "    \"low-income living\", \"lack of basic amenities\", \"homelessness\",\n",
        "    \"underprivileged conditions\", \"slum life\", \"subsistence living\"\n",
        "]\n",
        "\n",
        "# Sentence_template\n",
        "sentence_template = \"Lifestyle characterized by {}\"\n",
        "\n",
        "\n",
        "luxury = [sentence_template.format(concept) for concept in new_luxury_concepts_raw]\n",
        "\n",
        "poverty = [sentence_template.format(concept) for concept in new_poverty_concepts_raw]\n",
        "\n",
        "prefix_currency_luxury = \"Currency\"\n",
        "\n",
        "print(len(luxury),luxury)\n",
        "print(len(poverty),poverty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H9N8hmhxBHQ"
      },
      "source": [
        "# Concept 2 : Lawful vs Malice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le1UdDPE44O8"
      },
      "outputs": [],
      "source": [
        "# Concept List\n",
        "law_money =  [ \"clean currency\", \"trustworthy currency\",\"clean assets\",\"legal money\",'regulated capital','lawful money','trustworthy money','authorized funds','legitimate funds']\n",
        "\n",
        "terror_money =[\"terrorism currency\",\"cross-border terrorism currency\", \"insurgent's money\",\\\n",
        "\n",
        "\"terrorist financing\",\n",
        "    \"illicit funds\",\n",
        "    \"terror finance\",\n",
        "    \"blood money\",\n",
        "    \"dark capital\",\n",
        "    'terror funds'\n",
        "]\n",
        "\n",
        "\n",
        "print(len(law_money), len(terror_money))\n",
        "\n",
        "# Sentence_template\n",
        "sentence_template = \"{}\"\n",
        "\n",
        "\n",
        "law = [sentence_template.format(concept) for concept in law_money]\n",
        "terror = [sentence_template.format(concept) for concept in terror_money]\n",
        "\n",
        "prefix_currency_terror = \"Money kept in\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSf79f6f10yI"
      },
      "outputs": [],
      "source": [
        "def embed_currency(model=None, prefix_currency=None):\n",
        "  currency_embeddings_map = {}\n",
        "  print('prefix_currency ', prefix_currency)\n",
        "  assert prefix_currency is not None\n",
        "  for currency_code, currency_representation_string in currency_terms.items():\n",
        "      term_embeddings = []\n",
        "      sentence_to_embed = f\"{prefix_currency} {currency_representation_string}\"\n",
        "      print('sentence_to_embed ', sentence_to_embed)\n",
        "      embedding = model.encode([sentence_to_embed])[0]\n",
        "      term_embeddings.append(embedding)\n",
        "      currency_embeddings_map[currency_code] = term_embeddings\n",
        "\n",
        "  print(\"First 3 entries of currency_embeddings_map (showing shape and a few values):\")\n",
        "  count = 0\n",
        "  for currency_code, embeddings in currency_embeddings_map.items():\n",
        "      if count >= 3:\n",
        "          break\n",
        "      print(f\"  {currency_code}: {len(embeddings)} terms, first embedding shape: {embeddings[0].shape}, example values: {embeddings[0][:5]}\")\n",
        "      count += 1\n",
        "\n",
        "  print(\"\\nTotal number of currencies:\", len(currency_embeddings_map))\n",
        "  return currency_embeddings_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVQ3YsMP5KdX"
      },
      "outputs": [],
      "source": [
        "\n",
        "concept_type=\"luxury\"\n",
        "# concept_type =\"trust\"\n",
        "# concept_type=\"terrorism\"\n",
        "\n",
        "if concept_type =='terrorism':\n",
        "    concepts_dict = {'c_ter_mon':{'law_mon':law, 'ter_mon':terror}}\n",
        "\n",
        "\n",
        "if concept_type =='luxury':\n",
        "\n",
        "    concepts_dict = {'c_econ':{\"lux_pov\":luxury, \"poverty\":poverty}}\n",
        "\n",
        "if concept_type =='trust':\n",
        "    concepts_dict = {'c_trust':{\"trustworthy\":trustworthy, \"untrustworthy\":untrustworthy}}\n",
        "\n",
        "\n",
        "if concept_type =='terrorism':\n",
        "    prefix_currency = prefix_currency_terror\n",
        "if concept_type =='luxury':\n",
        "    prefix_currency = prefix_currency_luxury\n",
        "# if concept_type =='trust':\n",
        "#     prefix_currency = prefix_currency_luxury\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1cCoeV8iLxK"
      },
      "outputs": [],
      "source": [
        "prefix_currency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6adstDH5iLxK"
      },
      "outputs": [],
      "source": [
        "concept_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpCRFBAwLM6b"
      },
      "outputs": [],
      "source": [
        "print(concepts_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjPil4IRCSOS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import display\n",
        "\n",
        "def analyze_concept_similarity(concept_list_1, concept_list_2, name_1, name_2, model, currency_embeddings_map, currency_terms, current_model_name):\n",
        "\n",
        "\n",
        "    combined_concepts = concept_list_1 + concept_list_2\n",
        "    print(f\"\\n--- Analyzing '{name_1}' vs '{name_2}' ---\")\n",
        "    print(f\"Combined concepts for {name_1} and {name_2}: {len(combined_concepts)}\")\n",
        "\n",
        "    sentence_based_combined_concepts = combined_concepts\n",
        "\n",
        "    # Encoding concepts\n",
        "    combined_concept_embeddings = model.encode(sentence_based_combined_concepts)\n",
        "    currency_concept_rankings = {}\n",
        "\n",
        "    for currency_code, terms_list in currency_terms.items():\n",
        "        current_currency_embeddings = currency_embeddings_map.get(currency_code)\n",
        "\n",
        "        if not current_currency_embeddings:\n",
        "            print(f\"No embeddings found for {currency_code}. Skipping.\")\n",
        "            continue\n",
        "        concept_avg_similarities = {}\n",
        "        current_currency_embeddings_array = np.array(current_currency_embeddings)\n",
        "\n",
        "\n",
        "        for i, concept_term in enumerate(combined_concepts):\n",
        "            concept_embedding = combined_concept_embeddings[i]\n",
        "            similarities_to_currency_terms = cosine_similarity(concept_embedding.reshape(1, -1), current_currency_embeddings_array).flatten()\n",
        "            avg_similarity = np.mean(similarities_to_currency_terms)\n",
        "            concept_avg_similarities[concept_term] = avg_similarity\n",
        "\n",
        "        # Sort the concepts by similarity in descending order\n",
        "        sorted_concepts = sorted(concept_avg_similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "        currency_concept_rankings[currency_code] = sorted_concepts\n",
        "\n",
        "    currency_average_ranks = {}\n",
        "\n",
        "    for currency_code, ranked_concepts in currency_concept_rankings.items():\n",
        "        list1_ranks = []\n",
        "        list2_ranks = []\n",
        "\n",
        "        for rank_idx, (concept_term, similarity) in enumerate(ranked_concepts):\n",
        "            current_rank = rank_idx + 1\n",
        "\n",
        "            if concept_term in concept_list_1:\n",
        "                list1_ranks.append(current_rank)\n",
        "            elif concept_term in concept_list_2:\n",
        "                list2_ranks.append(current_rank)\n",
        "\n",
        "        avg_list1_rank = np.mean(list1_ranks) if list1_ranks else np.nan\n",
        "        med_list1_rank = np.median(list1_ranks) if list1_ranks else np.nan\n",
        "        avg_list2_rank = np.mean(list2_ranks) if list2_ranks else np.nan\n",
        "        med_list2_rank = np.median(list2_ranks) if list2_ranks else np.nan\n",
        "\n",
        "        currency_average_ranks[currency_code] = {\n",
        "            f'avg_{name_1}_rank': avg_list1_rank,\n",
        "            f'med_{name_1}_rank': med_list1_rank,\n",
        "            f'avg_{name_2}_rank': avg_list2_rank,\n",
        "            f'med_{name_2}_rank': med_list2_rank\n",
        "        }\n",
        "\n",
        "    # Creating a dataframe\n",
        "    avg_ranks_df = pd.DataFrame.from_dict(currency_average_ranks, orient='index')\n",
        "    avg_ranks_df.index.name = 'currency_code'\n",
        "    avg_ranks_df.insert(0, 'currency_value', avg_ranks_df.index.map(currency_terms))\n",
        "    avg_ranks_df.insert(1, 'model_name', current_model_name)\n",
        "    avg_ranks_df[f'avg_rank_diff ({name_2} - {name_1})'] = avg_ranks_df[f'avg_{name_2}_rank'] - avg_ranks_df[f'avg_{name_1}_rank']\n",
        "    avg_ranks_df[f'med_rank_diff ({name_2} - {name_1})'] = avg_ranks_df[f'med_{name_2}_rank'] - avg_ranks_df[f'med_{name_1}_rank']\n",
        "    col_mapping = {\n",
        "        f'avg_rank_diff ({name_2} - {name_1})': f'avg_rank_diff\\n({name_2} - {name_1})',\n",
        "        f'med_rank_diff ({name_2} - {name_1})': f'med_rank_diff\\n({name_2} - {name_1})',\n",
        "    }\n",
        "    avg_ranks_df = avg_ranks_df.rename(columns=col_mapping)\n",
        "\n",
        "    print(f\"--- Average and Median Ranks of {name_1} and {name_2} Concepts per Currency ---\")\n",
        "    print(\"A smaller rank means higher similarity/association.\\n\")\n",
        "\n",
        "    return avg_ranks_df.sort_values(by=f'med_rank_diff\\n({name_2} - {name_1})'), currency_concept_rankings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJtGK6fS0999"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoH03vbW4_da"
      },
      "outputs": [],
      "source": [
        "model_names_to_test_finsts = [\"FinLang/finance-embeddings-investopedia\",\"intfloat/multilingual-e5-large\", \"intfloat/multilingual-e5-large-instruct\",\"Alibaba-NLP/gte-large-en-v1.5\",\"Alibaba-NLP/gte-multilingual-base\",\"jinaai/jina-embeddings-v3\",\\\n",
        "                              \"jinaai/jina-embeddings-v2-base-en\",\"BAAI/bge-m3\",\"Snowflake/snowflake-arctic-embed-m-v1.5\",\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\"sentence-transformers/all-mpnet-base-v2\",\\\n",
        "                              \"Qwen/Qwen3-Embedding-0.6B\",\"Qwen/Qwen3-Embedding-4B\",\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\",\"google/embeddinggemma-300M\",]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp7VLadeiLxM"
      },
      "outputs": [],
      "source": [
        "model_names_to_test_finsts =[ \"FinLang/finance-embeddings-investopedia\"]\n",
        "model_names_to_test_finsts+=[\"Alibaba-NLP/gte-large-en-v1.5\",\"Alibaba-NLP/gte-multilingual-base\",\\\n",
        "                              \"jinaai/jina-embeddings-v2-base-en\",\"BAAI/bge-m3\",\"Snowflake/snowflake-arctic-embed-m-v1.5\",\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\"sentence-transformers/all-mpnet-base-v2\",\\\n",
        "                              \"Qwen/Qwen3-Embedding-0.6B\",\"google/embeddinggemma-300M\", \"intfloat/multilingual-e5-large-instruct\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4_GNHfLiLxM"
      },
      "outputs": [],
      "source": [
        "sorted(model_names_to_test_finsts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvEmBbz3iLxM"
      },
      "outputs": [],
      "source": [
        "len(model_names_to_test_finsts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxRh7zcduV45"
      },
      "source": [
        "# Running for all models except Finbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlKttUZA_5Yg"
      },
      "outputs": [],
      "source": [
        "output_dir = 'currency_biases'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6dUxljPiLxN"
      },
      "outputs": [],
      "source": [
        "concepts_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM7PT4LP4_9z",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "dict_all_biases = {}\n",
        "\n",
        "for current_model_name in model_names_to_test_finsts:\n",
        "    try:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing model: {current_model_name}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        model = SentenceTransformer(current_model_name, trust_remote_code=True)\n",
        "        dict_all_biases[current_model_name] = {}\n",
        "\n",
        "        for using_currency_keyword in ['currency_name_dict', 'currency_acronym_dict', 'currency_symbol_dict']:\n",
        "            print(f\"\\nAnalyzing with currency representation: {using_currency_keyword}\")\n",
        "            currency_terms = currency_terms_dict[using_currency_keyword]\n",
        "            dict_all_biases[current_model_name][using_currency_keyword] = {}\n",
        "            print(f\"\\n  Loading model: {current_model_name}\")\n",
        "            currency_embeddings_map = embed_currency(model=model, prefix_currency=prefix_currency)\n",
        "\n",
        "            for concept_group_name, concept_group_data in concepts_dict.items():\n",
        "                print(f\"\\n  Analyzing concept group: {concept_group_name}\")\n",
        "                if len(concept_group_data) == 2:\n",
        "                    names = list(concept_group_data.keys())\n",
        "                    concept_list_1 = concept_group_data[names[0]]\n",
        "                    concept_list_2 = concept_group_data[names[1]]\n",
        "                    name_1 = names[0]\n",
        "                    name_2 = names[1]\n",
        "\n",
        "                    df_bias, currency_concept_rankings = analyze_concept_similarity(\n",
        "                        concept_list_1, concept_list_2, name_1, name_2, model,\n",
        "                        currency_embeddings_map, currency_terms, current_model_name\n",
        "                    )\n",
        "                    dict_all_biases[current_model_name][using_currency_keyword][concept_group_name] = (df_bias, currency_concept_rankings)\n",
        "                    print(f\"\\n  Results for {concept_group_name} with {using_currency_keyword} for {current_model_name}:\")\n",
        "                    display(df_bias)\n",
        "                else:\n",
        "                    print(f\"  Skipping concept group '{concept_group_name}' as it does not contain exactly two lists for comparison.\")\n",
        "            file_name = f'dict_all_biases_{concept_type}_paper_{current_model_name.replace(\"/\",\"_\")}.pkl'\n",
        "            print('file_name for saving', file_name)\n",
        "            full_path = os.path.join(output_dir, file_name)\n",
        "            with open(full_path, 'wb') as f:\n",
        "                pickle.dump(dict_all_biases[current_model_name], f)\n",
        "            print(f\"\\nSuccessfully saved analysis results for model '{current_model_name}' to '{full_path}'\")\n",
        "            del currency_embeddings_map\n",
        "        del model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing model '{current_model_name}': {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nW6LQ0yiLxO"
      },
      "outputs": [],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ace02ad"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch accelerate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEfZIyqc0ogE"
      },
      "source": [
        "## FINBERT PIPELINE (We run Finbert separately since it isn't present in Sentence Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "384a08d8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers.pipelines import Pipeline\n",
        "from transformers.pipelines import PIPELINE_REGISTRY\n",
        "\n",
        "class FinBERTPipeline(Pipeline):\n",
        "    def _sanitize_parameters(self, **kwargs):\n",
        "        preprocess_kwargs = {}\n",
        "        if \"max_length\" in kwargs:\n",
        "            preprocess_kwargs[\"max_length\"] = kwargs[\"max_length\"]\n",
        "        return {}, preprocess_kwargs, {}\n",
        "\n",
        "    def preprocess(self, inputs, **kwargs):\n",
        "        tokenizer = self.tokenizer\n",
        "        return tokenizer(inputs, return_tensors=self.framework, padding=True, truncation=True, **kwargs)\n",
        "\n",
        "    def _forward(self, model_inputs, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**model_inputs, output_hidden_states=True)\n",
        "            cls_token_embeddings = outputs.hidden_states[-1][:, 0, :].cpu().numpy()\n",
        "        return {\"embeddings\": cls_token_embeddings}\n",
        "\n",
        "    def postprocess(self, model_outputs, **kwargs):\n",
        "        return model_outputs[\"embeddings\"]\n",
        "\n",
        "PIPELINE_REGISTRY.register_pipeline(\n",
        "    \"finbert-embedding\",\n",
        "    pipeline_class=FinBERTPipeline,\n",
        "    pt_model=AutoModelForSequenceClassification,\n",
        "    default={\"model\": \"ProsusAI/finbert\", \"revision\": \"main\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "398fe2f0"
      },
      "outputs": [],
      "source": [
        "def embed_currency_with_finbert_pipeline(pipe, currency_terms, prefix_currency):\n",
        "    currency_embeddings_map = {}\n",
        "    inputs = []\n",
        "    currency_codes_order = []\n",
        "\n",
        "    for currency_code, currency_representation_string in currency_terms.items():\n",
        "        sentence_to_embed = f\"{prefix_currency} {currency_representation_string}\"\n",
        "        print('sentence_to_embed', sentence_to_embed)\n",
        "        inputs.append(sentence_to_embed)\n",
        "        currency_codes_order.append(currency_code)\n",
        "\n",
        "    print(f\"Generating embeddings for {len(inputs)} currency terms using FinBERT pipeline...\")\n",
        "    all_embeddings_list = pipe(inputs, padding=True, truncation=True, max_length=128)\n",
        "    for i, currency_code in enumerate(currency_codes_order):\n",
        "        currency_embeddings_map[currency_code] = all_embeddings_list[i].reshape(1, -1)\n",
        "\n",
        "    print(\"FinBERT Currency embeddings map created successfully.\\n\")\n",
        "    print(\"First 3 entries of currency_embeddings_map (showing shape and a few values):\")\n",
        "    count = 0\n",
        "    for currency_code, embeddings in currency_embeddings_map.items():\n",
        "        if count >= 3:\n",
        "            break\n",
        "        print(f\"  {currency_code}: embedding shape: {embeddings.shape}, example values: {embeddings[0][:5]}\")\n",
        "        count += 1\n",
        "\n",
        "    print(\"\\nTotal number of currencies:\", len(currency_embeddings_map))\n",
        "    return currency_embeddings_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VouJy0xVTKBB"
      },
      "outputs": [],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4d02175",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "output_dir = '/outputs/currency_biases'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "dict_all_biases = {}\n",
        "\n",
        "model_names_to_test_finsts=[\"ProsusAI/finbert\"]\n",
        "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "for current_model_name in model_names_to_test_finsts:\n",
        "    try:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing model: {current_model_name}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        dict_all_biases[current_model_name] = {}\n",
        "\n",
        "        for using_currency_keyword in ['currency_name_dict', 'currency_acronym_dict', 'currency_symbol_dict']:\n",
        "\n",
        "            print(f\"\\nAnalyzing with currency representation: {using_currency_keyword}\")\n",
        "            currency_terms = currency_terms_dict[using_currency_keyword]\n",
        "            dict_all_biases[current_model_name][using_currency_keyword] = {}\n",
        "\n",
        "            print(f\"\\n  Loading model: {current_model_name}\")\n",
        "            model = None\n",
        "            finbert_pipe = None\n",
        "            currency_embeddings_map = None\n",
        "\n",
        "            if current_model_name == \"ProsusAI/finbert\":\n",
        "                finbert_model = AutoModelForSequenceClassification.from_pretrained(current_model_name).to(device)\n",
        "                finbert_tokenizer = AutoTokenizer.from_pretrained(current_model_name)\n",
        "                finbert_pipe = pipeline(\n",
        "                    \"finbert-embedding\",\n",
        "                    model=finbert_model,\n",
        "                    tokenizer=finbert_tokenizer,\n",
        "                    device=device )\n",
        "                print(f\"  Loaded FinBERT model and pipeline on {device}.\")\n",
        "                currency_embeddings_map = embed_currency_with_finbert_pipeline(finbert_pipe, currency_terms, prefix_currency=prefix_currency)\n",
        "            else:\n",
        "                model = SentenceTransformer(current_model_name, trust_remote_code=True).to(device)\n",
        "                print(f\"  Loaded SentenceTransformer model on {device}.\")\n",
        "                currency_embeddings_map = embed_currency(model=model,  prefix_currency=prefix_currency)\n",
        "            for concept_group_name, concept_group_data in concepts_dict.items():\n",
        "\n",
        "                print(f\"\\n  Analyzing concept group: {concept_group_name}\")\n",
        "                if len(concept_group_data) == 2:\n",
        "                    names = list(concept_group_data.keys())\n",
        "                    concept_list_1 = concept_group_data[names[0]]\n",
        "                    concept_list_2 = concept_group_data[names[1]]\n",
        "                    name_1 = names[0]\n",
        "                    name_2 = names[1]\n",
        "\n",
        "                    if current_model_name == \"ProsusAI/finbert\":\n",
        "                        class FinBERTConceptEncoder:\n",
        "                            def __init__(self, finbert_pipe):\n",
        "                                self.pipe = finbert_pipe\n",
        "\n",
        "                            def encode(self, sentences):\n",
        "                                embeddings_list = self.pipe(sentences, padding=True, truncation=True, max_length=128)\n",
        "                                return np.array(embeddings_list)\n",
        "\n",
        "                        concept_encoder = FinBERTConceptEncoder(finbert_pipe)\n",
        "                        df_bias, currency_concept_rankings = analyze_concept_similarity(\n",
        "                            concept_list_1, concept_list_2, name_1, name_2, concept_encoder,\n",
        "                            currency_embeddings_map, currency_terms, current_model_name\n",
        "                        )\n",
        "                    else:\n",
        "                        df_bias, currency_concept_rankings = analyze_concept_similarity(\n",
        "                            concept_list_1, concept_list_2, name_1, name_2, model,\n",
        "                            currency_embeddings_map, currency_terms, current_model_name\n",
        "                        )\n",
        "                    dict_all_biases[current_model_name][using_currency_keyword][concept_group_name] = (df_bias, currency_concept_rankings)\n",
        "                    print(f\"\\n  Results for {concept_group_name} with {using_currency_keyword} for {current_model_name}:\")\n",
        "                    display(df_bias)\n",
        "\n",
        "                else:\n",
        "                    print(f\"  Skipping concept group '{concept_group_name}' as it does not contain exactly two lists for comparison.\")\n",
        "\n",
        "            file_name = f'dict_all_biases_{concept_type}_paper_{current_model_name.replace(\"/\", \"_\")}.pkl'\n",
        "            full_path = os.path.join(output_dir, file_name)\n",
        "            print('full_path', full_path)\n",
        "            with open(full_path, 'wb') as f:\n",
        "                pickle.dump(dict_all_biases[current_model_name], f)\n",
        "            print(f\"\\nSuccessfully saved analysis results for model '{current_model_name}' to '{full_path}'\")\n",
        "\n",
        "            if model is not None:\n",
        "                del model\n",
        "            if finbert_pipe is not None:\n",
        "                del finbert_pipe\n",
        "                del finbert_model\n",
        "                del finbert_tokenizer\n",
        "            if currency_embeddings_map is not None:\n",
        "                del currency_embeddings_map\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                print(\"  CUDA cache emptied.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError processing model '{current_model_name}': {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        if 'model' in locals() and model is not None:\n",
        "            del model\n",
        "        if 'finbert_pipe' in locals() and finbert_pipe is not None:\n",
        "            del finbert_pipe\n",
        "            del finbert_model\n",
        "            del finbert_tokenizer\n",
        "        if 'currency_embeddings_map' in locals() and currency_embeddings_map is not None:\n",
        "            del currency_embeddings_map\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(\"  CUDA cache emptied after error.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Analysing dataframes"
      ],
      "metadata": {
        "id": "iDnvWnrbEbC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/outputs/currency_biases'"
      ],
      "metadata": {
        "id": "KWMkLkLwiiz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4970e83",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "all_consolidated_dfs = []\n",
        "\n",
        "\n",
        "luxury_file_prefix = 'dict_all_biases_luxury_paper_'\n",
        "\n",
        "\n",
        "for filename in os.listdir(output_dir):\n",
        "\n",
        "    if filename.startswith(luxury_file_prefix) and filename.endswith('.pkl'):\n",
        "        full_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        model_name_encoded = filename[len(luxury_file_prefix):-len('.pkl')]\n",
        "        current_model_name = model_name_encoded.replace('_', '/')\n",
        "\n",
        "        try:\n",
        "            with open(full_path, 'rb') as f:\n",
        "                model_bias_data = pickle.load(f)\n",
        "            print(f\"Successfully loaded data for model: {current_model_name}\")\n",
        "\n",
        "            for using_currency_keyword in ['currency_name_dict', 'currency_acronym_dict', 'currency_symbol_dict']:\n",
        "                if using_currency_keyword in model_bias_data and 'c_econ' in model_bias_data[using_currency_keyword]:\n",
        "                    df_bias = model_bias_data[using_currency_keyword]['c_econ'][0].copy()\n",
        "                    df_bias['currency_code'] = df_bias.index\n",
        "                    df_bias['model_name'] = current_model_name\n",
        "                    df_bias['currency_representation_type'] = using_currency_keyword\n",
        "                    all_consolidated_dfs.append(df_bias)\n",
        "                else:\n",
        "                    print(f\"  Skipping c_econ for {current_model_name} with {using_currency_keyword}: Data not found.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Pickle file not found for model: {current_model_name}. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing {current_model_name}: {e}. Skipping.\")\n",
        "\n",
        "if all_consolidated_dfs:\n",
        "    consolidated_luxury_bias_df = pd.concat(all_consolidated_dfs, ignore_index=True)\n",
        "    print(\"\\nConsolidated DataFrame created successfully.\")\n",
        "    print(f\"Total entries in consolidated_luxury_bias_df: {len(consolidated_luxury_bias_df)}\")\n",
        "    print(\"First 5 rows of the consolidated_luxury_bias_df:\")\n",
        "    display(consolidated_luxury_bias_df.head())\n",
        "    print(\"\\nInformation about consolidated_luxury_bias_df:\")\n",
        "    consolidated_luxury_bias_df.info()\n",
        "else:\n",
        "    print(\"No luxury bias data was consolidated. all_consolidated_dfs is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd01ab02"
      },
      "outputs": [],
      "source": [
        "for (model_name, currency_type), group_df in consolidated_luxury_bias_df.groupby(['model_name', 'currency_representation_type']):\n",
        "    print(f\"\\n--- Displaying Sorted DataFrame for Model: {model_name}, Currency Representation: {currency_type} ---\")\n",
        "    sorted_df = group_df.sort_values(by='med_rank_diff\\n(poverty - lux_pov)').reset_index(drop=True)\n",
        "    display(sorted_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fonts-noto-core fonts-noto-ui-core fonts-noto-extra fonts-noto-unhinted fonts-noto-cjk"
      ],
      "metadata": {
        "id": "2sCFm4HLlBbN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import shutil\n",
        "import os\n",
        "cache_dir = matplotlib.get_cachedir()\n",
        "if os.path.exists(cache_dir):\n",
        "    shutil.rmtree(cache_dir)\n",
        "    print(\"Font cache cleared. PLEASE RESTART RUNTIME NOW.\")"
      ],
      "metadata": {
        "id": "-hZ4mvPSlDnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir"
      ],
      "metadata": {
        "id": "V9xS_DXVhKI8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fonts-noto-core fonts-noto-extra fonts-noto-ui-core"
      ],
      "metadata": {
        "id": "S5xiZuGAlrM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heatmap Generation"
      ],
      "metadata": {
        "id": "Qz3Z3tn9iAAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "try:\n",
        "    plt.rcParams['font.sans-serif'] = ['Noto Sans', 'Noto Sans Display', 'Arial Unicode MS', 'DejaVu Sans', 'Noto Sans CJK JP', 'sans-serif', 'Noto Color Emoji']\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    fm._load_fontmanager(try_read_cache=False)\n",
        "    print(\"Matplotlib font configuration updated within heatmap cell.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not set advanced font settings within heatmap cell: {e}. Falling back to default.\")\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "output_dir_lux = output_dir+\"/split_heatmaps_lux/\"\n",
        "print('output_dir_lux', output_dir_lux)\n",
        "os.makedirs(output_dir_lux, exist_ok=True)\n",
        "currency_types = consolidated_luxury_bias_df['currency_representation_type'].unique()\n",
        "currency_type_captions = {\n",
        "    'currency_name_dict': 'Currency Name',\n",
        "    'currency_acronym_dict': 'Currency Acronym',\n",
        "    'currency_symbol_dict': 'Currency Symbol'\n",
        "}\n",
        "\n",
        "\n",
        "symbol_display_map = {}\n",
        "for code, symbol in currency_symbol_dict.items():\n",
        "    if code in currency_name_dict:\n",
        "        symbol_display_map[symbol] = f\"{symbol} ({currency_name_dict[code]})\"\n",
        "\n",
        "\n",
        "acronym_display_map = {}\n",
        "for code, acronym in currency_acronym_dict.items():\n",
        "    if code in currency_name_dict:\n",
        "        acronym_display_map[acronym] = f\"{acronym} ({currency_name_dict[code]})\"\n",
        "\n",
        "def generate_split_heatmap(df, title_suffix, filename_suffix, currency_representation_type):\n",
        "    \"\"\"Helper function to plot a segment of the pivot table.\"\"\"\n",
        "    num_currencies = len(df)\n",
        "    fig_height = max(6, num_currencies * 0.4)\n",
        "\n",
        "    plt.figure(figsize=(16, fig_height))\n",
        "    ax = sns.heatmap(\n",
        "        df,\n",
        "        cmap='coolwarm',\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        linewidths=.5,\n",
        "        linecolor='black',\n",
        "        cbar_kws={\n",
        "            'label': 'Median Rank Difference (Poverty - Luxury)',\n",
        "            'shrink': 0.1\n",
        "        },\n",
        "        # annot_kws={'fontsize': 10}\n",
        "        annot_kws={'fontsize': 14, 'weight': 'bold'}\n",
        "        # annot_kws={'fontsize': 10}\n",
        "\n",
        "    )\n",
        "\n",
        "    plt.title(f'Luxury & Poverty Bias Heatmap: {title_suffix}', fontsize=18, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "\n",
        "    if currency_representation_type == 'currency_symbol_dict':\n",
        "        original_y_labels = df.index.tolist()\n",
        "        new_y_labels = [symbol_display_map.get(label, label) for label in original_y_labels]\n",
        "        ax.set_yticklabels(new_y_labels, rotation=0)\n",
        "        ax.tick_params(axis='y', labelsize=10)\n",
        "    elif currency_representation_type == 'currency_acronym_dict':\n",
        "        original_y_labels = df.index.tolist()\n",
        "        new_y_labels = [acronym_display_map.get(label, label) for label in original_y_labels]\n",
        "        ax.set_yticklabels(new_y_labels, rotation=0)\n",
        "        ax.tick_params(axis='y', labelsize=14)\n",
        "    else:\n",
        "        ax.tick_params(axis='y', labelsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = os.path.join(output_dir_lux, f\"heatmap_{filename_suffix}_lux_pov.pdf\")\n",
        "    plt.savefig(save_path, format=\"pdf\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Saved: {save_path}\")\n",
        "\n",
        "for currency_type in currency_types:\n",
        "    subset_df = consolidated_luxury_bias_df[\n",
        "        (consolidated_luxury_bias_df['currency_representation_type'] == currency_type) &\n",
        "        (consolidated_luxury_bias_df['model_name'] != 'Qwen/Qwen3-Embedding-4B')\n",
        "    ].copy()\n",
        "\n",
        "    if subset_df.empty: continue\n",
        "    pivot_table = subset_df.pivot_table(\n",
        "        index='currency_value',\n",
        "        columns='model_name',\n",
        "        values='med_rank_diff\\n(poverty - lux_pov)'\n",
        "    )\n",
        "    pivot_table.columns = [col.split('/')[-1] for col in pivot_table.columns]\n",
        "    pivot_table = pivot_table.loc[pivot_table.mean(axis=1).sort_values().index]\n",
        "    mid_point = len(pivot_table) // 2\n",
        "    part1 = pivot_table.iloc[:mid_point]\n",
        "    part2 = pivot_table.iloc[mid_point:]\n",
        "\n",
        "    descriptive_name = currency_type_captions.get(currency_type, currency_type)\n",
        "\n",
        "    generate_split_heatmap(part1, f\"{descriptive_name} (Part 1/2)\", f\"{currency_type}_pt1\", currency_type)\n",
        "    generate_split_heatmap(part2, f\"{descriptive_name} (Part 2/2)\", f\"{currency_type}_pt2\", currency_type)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B9eb5ysChKHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_table"
      ],
      "metadata": {
        "id": "GPcCXYcZjLXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "try:\n",
        "    plt.rcParams['font.sans-serif'] = ['Noto Sans Display', 'Arial Unicode MS', 'Noto Color Emoji', 'DejaVu Sans', 'Noto Sans CJK JP', 'sans-serif']\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "    print(\"Matplotlib font configuration updated.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not set advanced font settings: {e}. Falling back to default.\")\n"
      ],
      "metadata": {
        "id": "0z0VASt9iPcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing Model Aggregates"
      ],
      "metadata": {
        "id": "frDnjVkwiagc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f059a763"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "aggregated_bias_metrics = []\n",
        "\n",
        "for (model_name, currency_type), group_df in consolidated_luxury_bias_df.groupby(['model_name', 'currency_representation_type']):\n",
        "\n",
        "    rank_diffs = group_df['med_rank_diff\\n(poverty - lux_pov)']\n",
        "    median_of_rank_diffs = np.median(rank_diffs)\n",
        "    absolute_deviations = np.abs(rank_diffs - median_of_rank_diffs)\n",
        "    avg_abs_deviation = np.mean(absolute_deviations)\n",
        "    standard_deviation = np.std(rank_diffs)\n",
        "    max_min_rank_diff = rank_diffs.max() - rank_diffs.min()\n",
        "\n",
        "    aggregated_bias_metrics.append({\n",
        "        'model_name': model_name,\n",
        "        'currency_representation_type': currency_type,\n",
        "        'median_rank_diff (poverty - lux_pov)': median_of_rank_diffs,\n",
        "        'standard_deviation': standard_deviation,\n",
        "        'avg_abs_deviation_from_median': avg_abs_deviation,\n",
        "        'max_min_rank_diff (poverty - lux_pov)': max_min_rank_diff\n",
        "    })\n",
        "\n",
        "aggregated_bias_df = pd.DataFrame(aggregated_bias_metrics)\n",
        "\n",
        "print(\"Aggregated Bias Metrics calculated successfully.\")\n",
        "print(\"First 5 rows of the aggregated_bias_df:\")\n",
        "display(aggregated_bias_df.head())\n",
        "\n",
        "print(\"\\nFull Aggregated Bias Metrics DataFrame (sorted by Median Rank Diff):\")\n",
        "display(aggregated_bias_df.sort_values(by='median_rank_diff (poverty - lux_pov)').reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "781c5999"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "currency_types = aggregated_bias_df['currency_representation_type'].unique()\n",
        "\n",
        "for currency_type in currency_types:\n",
        "    print(f\"\\n--- Summary Table for Currency Representation: {currency_type} ---\")\n",
        "    subset_df = aggregated_bias_df[\n",
        "        aggregated_bias_df['currency_representation_type'] == currency_type\n",
        "    ].copy()\n",
        "    sorted_subset_df = subset_df.sort_values(by='avg_abs_deviation_from_median').reset_index(drop=True)\n",
        "    latex_table = sorted_subset_df.to_latex(index=False)\n",
        "    print(latex_table)\n",
        "    display(sorted_subset_df.drop(columns=['currency_representation_type']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31f07e22"
      },
      "source": [
        "## Analyze Mean and Standard Deviation of Median Rank Differences Across Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e7ca0b9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "grouped_currencies = consolidated_luxury_bias_df.groupby(['currency_representation_type', 'currency_value'])['med_rank_diff\\n(poverty - lux_pov)']\n",
        "\n",
        "mean_std_rank_diff = grouped_currencies.agg(['mean', 'std']).reset_index()\n",
        "mean_std_rank_diff.rename(columns={'mean': 'mean_med_rank_diff (poverty - lux_pov)',\n",
        "                                   'std': 'std_dev_med_rank_diff (poverty - lux_pov)'}, inplace=True)\n",
        "\n",
        "print(\"Mean and Standard Deviation of Median Rank Differences calculated successfully.\\n\")\n",
        "\n",
        "for currency_type in mean_std_rank_diff['currency_representation_type'].unique():\n",
        "    print(f\"--- Analysis for Currency Representation Type: {currency_type} ---\")\n",
        "\n",
        "    subset_df = mean_std_rank_diff[mean_std_rank_diff['currency_representation_type'] == currency_type].copy()\n",
        "    sorted_by_mean = subset_df.sort_values(by='mean_med_rank_diff (poverty - lux_pov)', ascending=True)\n",
        "\n",
        "    print(\"\\nTop 5 Currencies (most poverty-biased, lowest mean_med_rank_diff):\")\n",
        "    display(sorted_by_mean.head(5))\n",
        "\n",
        "    print(\"\\nBottom 5 Currencies (most luxury-biased, highest mean_med_rank_diff):\")\n",
        "    display(sorted_by_mean.tail(5))\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9c0c71"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "currency_type_captions = {\n",
        "    'currency_name_dict': 'Currency Name',\n",
        "    'currency_acronym_dict': 'Currency Acronym',\n",
        "    'currency_symbol_dict': 'Currency Symbol'\n",
        "}\n",
        "\n",
        "latex_output = [\n",
        "    \"\\\\begin{table*}[ht!]\",\n",
        "    \"\\\\centering\",\n",
        "    \"\\\\caption{Consolidated Associative Bias: Top 5 (Poverty-Biased) and Bottom 5 (Luxury-Biased)}\",\n",
        "    \"\\\\label{tab:consolidated_bias}\",\n",
        "    \"\\\\small\",\n",
        "    \"\\\\begin{tabular}{lllc}\",\n",
        "    \"\\\\toprule\",\n",
        "    \"\\\\textbf{Representation Type} & \\\\textbf{Bias Group} & \\\\textbf{Currency} & \\\\textbf{Rank Diff ($Mean \\\\pm Std$)} \\\\\\\\\",\n",
        "    \"\\\\midrule\"\n",
        "]\n",
        "\n",
        "for currency_type in mean_std_rank_diff['currency_representation_type'].unique():\n",
        "    display_name = currency_type_captions.get(currency_type, currency_type)\n",
        "    subset_df = mean_std_rank_diff[mean_std_rank_diff['currency_representation_type'] == currency_type].copy()\n",
        "    sorted_df = subset_df.sort_values(by='mean_med_rank_diff (poverty - lux_pov)', ascending=True)\n",
        "    top_5 = sorted_df.head(5)\n",
        "    bottom_5 = sorted_df.tail(5)\n",
        "\n",
        "    for i, (_, row) in enumerate(top_5.iterrows()):\n",
        "        category = f\"\\\\multirow{{10}}{{*}}{{\\\\textbf{{{display_name}}}}}\" if i == 0 else \"\"\n",
        "        group = \"\\\\multirow{5}{*}{Top 5 (Pov)}\" if i == 0 else \"\"\n",
        "        stats = f\"{row['mean_med_rank_diff (poverty - lux_pov)']:.2f} $\\\\pm$ {row['std_dev_med_rank_diff (poverty - lux_pov)']:.2f}\"\n",
        "        latex_output.append(f\"{category} & {group} & {row['currency_value']} & {stats} \\\\\\\\\")\n",
        "\n",
        "    latex_output.append(\"\\\\cmidrule{2-4}\")\n",
        "    for i, (_, row) in enumerate(bottom_5.iterrows()):\n",
        "        group = \"\\\\multirow{5}{*}{Bottom 5 (Lux)}\" if i == 0 else \"\"\n",
        "        stats = f\"{row['mean_med_rank_diff (poverty - lux_pov)']:.2f} $\\\\pm$ {row['std_dev_med_rank_diff (poverty - lux_pov)']:.2f}\"\n",
        "        latex_output.append(f\" & {group} & {row['currency_value']} & {stats} \\\\\\\\\")\n",
        "\n",
        "    latex_output.append(\"\\\\midrule\")\n",
        "latex_output.append(\"\\\\bottomrule\")\n",
        "latex_output.append(\"\\\\end{tabular}\")\n",
        "latex_output.append(\"\\\\end{table*}\")\n",
        "\n",
        "print('\\n'.join(latex_output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consolidated_luxury_bias_df.head(10)"
      ],
      "metadata": {
        "id": "NPWU5oytmmsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consolidated_luxury_bias_df.head(3)"
      ],
      "metadata": {
        "id": "CV2onqj2-Uwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constructing Model Wise Plots"
      ],
      "metadata": {
        "id": "A6I1ha0gkMmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "fm._load_fontmanager(try_read_cache=False)\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = ['Noto Sans', 'Noto Color Emoji', 'DejaVu Sans', 'Arial Unicode MS', 'FreeSans', 'Symbola']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "currency_types = consolidated_luxury_bias_df['currency_representation_type'].unique()\n",
        "currency_type_captions = {\n",
        "    'currency_name_dict': 'Currency Name',\n",
        "    'currency_acronym_dict': 'Currency Acronym',\n",
        "    'currency_symbol_dict': 'Currency Symbol'\n",
        "}\n",
        "\n",
        "for i, currency_type in enumerate(currency_types):\n",
        "    descriptive_currency_type = currency_type_captions.get(currency_type, currency_type)\n",
        "\n",
        "    print(f\"\\n--- Generating Plot for Currency Representation: {descriptive_currency_type} ---\")\n",
        "    subset_df = consolidated_luxury_bias_df[\n",
        "        consolidated_luxury_bias_df['currency_representation_type'] == currency_type\n",
        "    ].copy()\n",
        "\n",
        "    subset_df['model_name'] = subset_df['model_name'].str.split('/').str[-1]\n",
        "    subset_df = subset_df.sort_values(by='model_name', ascending=True)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(18, 10))\n",
        "\n",
        "    sns.boxplot(\n",
        "        x='med_rank_diff\\n(poverty - lux_pov)',\n",
        "        y='model_name',\n",
        "        data=subset_df,\n",
        "        palette='viridis',\n",
        "        width=0.6,\n",
        "        showfliers=True,\n",
        "        flierprops={\"marker\": \"o\", \"markersize\": 8, \"alpha\": 0.6, \"markeredgewidth\": 2},\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    sns.pointplot(\n",
        "        x='med_rank_diff\\n(poverty - lux_pov)',\n",
        "        y='model_name',\n",
        "        data=subset_df,\n",
        "        linestyle='none',\n",
        "        color='black',\n",
        "        markers='D',\n",
        "        errorbar='sd',\n",
        "        capsize=0.1,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel('Median Rank Difference (Poverty - Luxury) ', fontsize=26, fontweight='bold')\n",
        "\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "\n",
        "    for label in ax.get_yticklabels():\n",
        "      label.set_fontweight('bold')\n",
        "      label.set_fontsize(30)\n",
        "\n",
        "    for label in ax.get_xticklabels():\n",
        "      label.set_fontweight('bold')\n",
        "      label.set_fontsize(20)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_filename = f\"model_luxury_bias_distribution_{currency_type}.pdf\"\n",
        "    plt.savefig(os.path.join(output_dir, plot_filename), format=\"pdf\", dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Plot saved as: {os.path.join(output_dir, plot_filename)}\")"
      ],
      "metadata": {
        "id": "O1n8vp_0dRhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmv-vdi2J7Pn"
      },
      "source": [
        "# Loading Terrorism Concept Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEbMzulvJ5bL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "output_dir = '/outputs/currency_biases'\n",
        "\n",
        "all_consolidated_dfs = []\n",
        "\n",
        "terrorism_file_prefix = 'dict_all_biases_terrorism_paper_'\n",
        "\n",
        "for filename in os.listdir(output_dir):\n",
        "    if filename.startswith(terrorism_file_prefix) and filename.endswith('.pkl'):\n",
        "        full_path = os.path.join(output_dir, filename)\n",
        "        model_name_encoded = filename[len(terrorism_file_prefix):-len('.pkl')]\n",
        "        current_model_name = model_name_encoded.replace('_', '/')\n",
        "        try:\n",
        "            with open(full_path, 'rb') as f:\n",
        "                model_bias_data = pickle.load(f)\n",
        "            print(f\"Successfully loaded data for model: {current_model_name}\")\n",
        "\n",
        "            for using_currency_keyword in ['currency_name_dict', 'currency_acronym_dict', 'currency_symbol_dict']:\n",
        "                if using_currency_keyword in model_bias_data and 'c_ter_mon' in model_bias_data[using_currency_keyword]:\n",
        "                    df_bias = model_bias_data[using_currency_keyword]['c_ter_mon'][0].copy()\n",
        "                    df_bias['currency_code'] = df_bias.index\n",
        "                    df_bias['model_name'] = current_model_name\n",
        "                    df_bias['currency_representation_type'] = using_currency_keyword\n",
        "                    all_consolidated_dfs.append(df_bias)\n",
        "                else:\n",
        "                    print(f\"  Skipping c_ter_mon for {current_model_name} with {using_currency_keyword}: Data not found.\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Pickle file not found for model: {current_model_name}. Skipping.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing {current_model_name}: {e}. Skipping.\")\n",
        "\n",
        "if all_consolidated_dfs:\n",
        "    consolidated_terrorism_bias_df = pd.concat(all_consolidated_dfs, ignore_index=True)\n",
        "    print(\"\\nConsolidated DataFrame created successfully.\")\n",
        "    print(f\"Total entries in consolidated_terrorism_bias_df: {len(consolidated_terrorism_bias_df)}\")\n",
        "    print(\"First 5 rows of the consolidated_terrorism_bias_df:\")\n",
        "    display(consolidated_terrorism_bias_df.head())\n",
        "    print(\"\\nInformation about consolidated_luxury_bias_df:\")\n",
        "    consolidated_terrorism_bias_df.info()\n",
        "else:\n",
        "    print(\"No luxury bias data was consolidated. all_consolidated_dfs is empty.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgMM74w0KI2U"
      },
      "outputs": [],
      "source": [
        "\n",
        "for (model_name, currency_type), group_df in consolidated_terrorism_bias_df.groupby(['model_name', 'currency_representation_type']):\n",
        "    print(f\"\\n--- Displaying Sorted DataFrame for Model: {model_name}, Currency Representation: {currency_type} ---\")\n",
        "    sorted_df = group_df.sort_values(by='med_rank_diff\\n(ter_mon - law_mon)').reset_index(drop=True)\n",
        "    display(sorted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Heatmaps"
      ],
      "metadata": {
        "id": "R6CuM0JBlgul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.font_manager as fm\n",
        "fm._load_fontmanager(try_read_cache=False)\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = ['Noto Sans', 'Noto Color Emoji', 'DejaVu Sans', 'Arial Unicode MS', 'FreeSans', 'Symbola']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "plt.rcParams.update({'font.size': 18})\n",
        "\n",
        "output_dir_terror = output_dir+\"terrorism_split_heatmaps/\"\n",
        "print('output_dir_terror', output_dir_terror)\n",
        "os.makedirs(output_dir_terror, exist_ok=True)\n",
        "\n",
        "\n",
        "currency_types = consolidated_terrorism_bias_df['currency_representation_type'].unique()\n",
        "currency_type_captions = {\n",
        "    'currency_name_dict': 'Currency Name',\n",
        "    'currency_acronym_dict': 'Currency Acronym',\n",
        "    'currency_symbol_dict': 'Currency Symbol'\n",
        "}\n",
        "\n",
        "symbol_display_map = {}\n",
        "for code, symbol in currency_symbol_dict.items():\n",
        "    if code in currency_name_dict:\n",
        "        symbol_display_map[symbol] = f\"{symbol} ({currency_name_dict[code]})\"\n",
        "\n",
        "acronym_display_map = {}\n",
        "for code, acronym in currency_acronym_dict.items():\n",
        "    if code in currency_name_dict:\n",
        "        acronym_display_map[acronym] = f\"{acronym} ({currency_name_dict[code]})\"\n",
        "\n",
        "def generate_split_heatmap(df, title_suffix, filename_suffix, currency_representation_type):\n",
        "    \"\"\"Helper function to plot a segment of the pivot table.\"\"\"\n",
        "    num_currencies = len(df)\n",
        "    fig_height = max(6, num_currencies * 0.4)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16, fig_height))\n",
        "\n",
        "\n",
        "    sns.heatmap(\n",
        "        df,\n",
        "        cmap='coolwarm',\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        linewidths=.5,\n",
        "        linecolor='black',\n",
        "        cbar_kws={\n",
        "            'label': 'Median Rank Difference (Terrorism - Lawfulness)',\n",
        "            'shrink': 0.25\n",
        "        },\n",
        "        annot_kws={'fontsize': 14, 'weight': 'bold'},\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    plt.title(f'Terrorism & Lawful Currency Bias Heatmap: {title_suffix}', fontsize=18, pad=20)\n",
        "    plt.xticks(rotation=45, ha='right', fontsize=14)\n",
        "\n",
        "    if currency_representation_type == 'currency_symbol_dict':\n",
        "        original_y_labels = df.index.tolist()\n",
        "        new_y_labels = [symbol_display_map.get(label, label) for label in original_y_labels]\n",
        "        ax.set_yticklabels(new_y_labels, rotation=0)\n",
        "        ax.tick_params(axis='y', labelsize=10)\n",
        "    elif currency_representation_type == 'currency_acronym_dict':\n",
        "        original_y_labels = df.index.tolist()\n",
        "        new_y_labels = [acronym_display_map.get(label, label) for label in original_y_labels]\n",
        "        ax.set_yticklabels(new_y_labels, rotation=0)\n",
        "        ax.tick_params(axis='y', labelsize=12)\n",
        "    else:\n",
        "        ax.tick_params(axis='y', labelsize=16)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    save_path = os.path.join(output_dir_terror, f\"heatmap_{filename_suffix}.pdf\")\n",
        "    plt.savefig(save_path, format=\"pdf\", bbox_inches='tight')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Saved: {save_path}\")\n",
        "\n",
        "for currency_type in currency_types:\n",
        "    subset_df = consolidated_terrorism_bias_df[\n",
        "        (consolidated_terrorism_bias_df['currency_representation_type'] == currency_type) &\n",
        "        (consolidated_terrorism_bias_df['model_name'] != 'Qwen/Qwen3-Embedding-4B')\n",
        "    ].copy()\n",
        "\n",
        "    if subset_df.empty: continue\n",
        "\n",
        "    pivot_table = subset_df.pivot_table(\n",
        "        index='currency_value',\n",
        "        columns='model_name',\n",
        "        values='med_rank_diff\\n(ter_mon - law_mon)'\n",
        "    )\n",
        "    pivot_table.columns = [col.split('/')[-1] for col in pivot_table.columns]\n",
        "    pivot_table = pivot_table.loc[pivot_table.mean(axis=1).sort_values().index]\n",
        "\n",
        "    mid_point = len(pivot_table) // 2\n",
        "    part1 = pivot_table.iloc[:mid_point]\n",
        "    part2 = pivot_table.iloc[mid_point:]\n",
        "\n",
        "    descriptive_name = currency_type_captions.get(currency_type, currency_type)\n",
        "\n",
        "    generate_split_heatmap(part1, f\"{descriptive_name} (Part 1/2)\", f\"{currency_type}_terrorism_pt1\", currency_type)\n",
        "    generate_split_heatmap(part2, f\"{descriptive_name} (Part 2/2)\", f\"{currency_type}_terrorism_pt2\", currency_type)\n",
        "\n"
      ],
      "metadata": {
        "id": "3cwKmClomPvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_std_rank_diff.head(3)"
      ],
      "metadata": {
        "id": "ijYoRKGwWYf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Box Plots for Terrorism Data"
      ],
      "metadata": {
        "id": "EfimP7LXly3-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "769efdad"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "fm._load_fontmanager(try_read_cache=False)\n",
        "\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.sans-serif'] = ['Noto Sans', 'Noto Color Emoji', 'DejaVu Sans', 'Arial Unicode MS', 'FreeSans', 'Symbola']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "currency_types = consolidated_terrorism_bias_df['currency_representation_type'].unique()\n",
        "\n",
        "currency_type_captions = {\n",
        "    'currency_name_dict': 'Currency Name',\n",
        "    'currency_acronym_dict': 'Currency Acronym',\n",
        "    'currency_symbol_dict': 'Currency Symbol'\n",
        "}\n",
        "\n",
        "for i, currency_type in enumerate(currency_types):\n",
        "    descriptive_currency_type = currency_type_captions.get(currency_type, currency_type)\n",
        "    print(f\"\\n--- Generating Plot for Currency Representation: {descriptive_currency_type} ---\")\n",
        "    subset_df = consolidated_terrorism_bias_df[\n",
        "        consolidated_terrorism_bias_df['currency_representation_type'] == currency_type\n",
        "    ].copy()\n",
        "    subset_df['short_model_name'] = subset_df['model_name'].str.split('/').str[-1]\n",
        "    subset_df = subset_df.sort_values(by='short_model_name', ascending=True)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(18, 10))\n",
        "    sns.boxplot(\n",
        "        x='med_rank_diff\\n(ter_mon - law_mon)',\n",
        "        y='short_model_name',\n",
        "        data=subset_df,\n",
        "        palette='viridis',\n",
        "        width=0.6,\n",
        "        showfliers=True,\n",
        "        flierprops={\"marker\": \"o\", \"markersize\": 8, \"alpha\": 0.6, \"markeredgewidth\": 2},\n",
        "        ax=ax\n",
        "    )\n",
        "    sns.pointplot(\n",
        "        x='med_rank_diff\\n(ter_mon - law_mon)',\n",
        "        y='short_model_name',\n",
        "        data=subset_df,\n",
        "        linestyle='none',\n",
        "        color='black',\n",
        "        markers='D',\n",
        "        errorbar='sd',\n",
        "        capsize=0.1,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(f'Median Rank Difference (Terrorism - Lawfulness)', fontsize=22, fontweight='bold')\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "\n",
        "    for label in ax.get_yticklabels():\n",
        "      label.set_fontweight('bold')\n",
        "      label.set_fontsize(30)\n",
        "\n",
        "    for label in ax.get_xticklabels():\n",
        "      label.set_fontweight('bold')\n",
        "      label.set_fontsize(20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_filename = f\"model_terrorism_bias_distribution_{currency_type}.pdf\"\n",
        "    plt.savefig(os.path.join(output_dir, plot_filename), format=\"pdf\", dpi=300)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Plot saved as: {os.path.join(output_dir, plot_filename)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813b6bdd"
      },
      "source": [
        "#Displaying Top/Bottom Biased Currencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d91b309e"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def find_and_display_extreme_biases(\n",
        "    consolidated_df: pd.DataFrame,\n",
        "    rank_diff_col: str,\n",
        "    bias_type_name: str,\n",
        "    positive_bias_label: str,\n",
        "    negative_bias_label: str,\n",
        "    generate_latex: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Identifies and displays the top 5 most and least biased currencies for each model\n",
        "    and currency representation type. Optionally generates LaTeX tables.\n",
        "\n",
        "    Args:\n",
        "        consolidated_df (pd.DataFrame): The DataFrame containing consolidated bias data.\n",
        "        rank_diff_col (str): The name of the column containing the median rank difference.\n",
        "        bias_type_name (str): A descriptive name for the bias type (e.g., 'Luxury', 'Terrorism').\n",
        "        positive_bias_label (str): Label for the positive bias direction (e.g., 'Poverty-Biased').\n",
        "        negative_bias_label (str): Label for the negative bias direction (e.g., 'Luxury-Biased').\n",
        "        generate_latex (bool): If True, prints LaTeX tables instead of displaying DataFrames.\n",
        "    \"\"\"\n",
        "\n",
        "    grouped_data = consolidated_df.groupby(['model_name', 'currency_representation_type'])\n",
        "\n",
        "    print(f\"\\n--- Analyzing {bias_type_name} Bias: Top 5 {negative_bias_label} and Bottom 5 {positive_bias_label} Currencies ---\")\n",
        "\n",
        "    for (model_name, currency_type), group_df in grouped_data:\n",
        "        if not generate_latex:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"Model: {model_name}, Representation: {currency_type}\")\n",
        "            print(f\"{'='*80}\")\n",
        "        sorted_df = group_df.sort_values(by=rank_diff_col).reset_index(drop=True)\n",
        "        top_5_negative = sorted_df.head(5)\n",
        "        bottom_5_positive = sorted_df.tail(5)\n",
        "\n",
        "        if generate_latex:\n",
        "            latex_caption_top = f\"Top 5 {negative_bias_label} Currencies for model {model_name} with {currency_type.replace('_dict', '').replace('currency_', '').replace('_', ' ')} representation.\"\n",
        "            print(f\"\\\\begin{{table*}}[ht!]\\n\\\\centering\\n\\\\caption{{{latex_caption_top}}}\\n\\\\label{{tab:{bias_type_name.lower().replace(' ', '_')}_{model_name.replace('/', '_')}_{currency_type}_top}}\\n\\\\begin{{tabular}}{{{'c' * len(top_5_negative[['currency_value', rank_diff_col]].columns)}}}\\n\\\\toprule\")\n",
        "            print(top_5_negative[['currency_value', rank_diff_col]].to_latex(index=False, header=True, escape=True))\n",
        "            print(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table*}\\n\")\n",
        "\n",
        "            latex_caption_bottom = f\"Bottom 5 {positive_bias_label} Currencies for model {model_name} with {currency_type.replace('_dict', '').replace('currency_', '').replace('_', ' ')} representation.\"\n",
        "            print(f\"\\\\begin{{table*}}[ht!]\\n\\\\centering\\n\\\\caption{{{latex_caption_bottom}}}\\n\\\\label{{tab:{bias_type_name.lower().replace(' ', '_')}_{model_name.replace('/', '_')}_{currency_type}_bottom}}\\n\\\\begin{{tabular}}{{{'c' * len(bottom_5_positive[['currency_value', rank_diff_col]].columns)}}}\\n\\\\toprule\")\n",
        "            print(bottom_5_positive[['currency_value', rank_diff_col]].to_latex(index=False, header=True, escape=True))\n",
        "            print(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table*}\\n\")\n",
        "\n",
        "        else:\n",
        "            print(f\"\\nTop 5 {negative_bias_label} Currencies (lowest '{rank_diff_col}'):\")\n",
        "            display(top_5_negative[['currency_value', rank_diff_col]])\n",
        "\n",
        "            print(f\"\\nBottom 5 {positive_bias_label} Currencies (highest '{rank_diff_col}'):\")\n",
        "            display(bottom_5_positive[['currency_value', rank_diff_col]])\n",
        "desired_currency_types = ['currency_name_dict', 'currency_acronym_dict']\n",
        "\n",
        "filtered_luxury_df = consolidated_luxury_bias_df[\n",
        "    consolidated_luxury_bias_df['currency_representation_type'].isin(desired_currency_types)\n",
        "].copy()\n",
        "\n",
        "filtered_terrorism_df = consolidated_terrorism_bias_df[\n",
        "    consolidated_terrorism_bias_df['currency_representation_type'].isin(desired_currency_types)\n",
        "].copy()\n",
        "\n",
        "print(\"\\n========== Generating LaTeX Tables for Luxury Bias (Currency Name and Acronym) ==========\")\n",
        "find_and_display_extreme_biases(\n",
        "    filtered_luxury_df,\n",
        "    'med_rank_diff\\n(poverty - lux_pov)',\n",
        "    'Luxury vs. Poverty',\n",
        "    'Poverty-Biased',\n",
        "    'Luxury-Biased',\n",
        "    generate_latex=True\n",
        ")\n",
        "print(\"\\n========== Generating LaTeX Tables for Terrorism Bias (Currency Name and Acronym) ==========\")\n",
        "find_and_display_extreme_biases(\n",
        "    filtered_terrorism_df,\n",
        "    'med_rank_diff\\n(ter_mon - law_mon)',\n",
        "    'Terrorism vs. Lawfulness',\n",
        "    'Terrorism-Biased',\n",
        "    'Lawfulness-Biased',\n",
        "    generate_latex=True\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Results"
      ],
      "metadata": {
        "id": "c-WKBRV520K4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fff2d497"
      },
      "source": [
        "snowflake_luxury_results = consolidated_luxury_bias_df[consolidated_luxury_bias_df['model_name'].str.contains('Snowflake')]\n",
        "\n",
        "print(\"\\n--- Luxury vs. Poverty Bias Results for Snowflake Model ---\")\n",
        "find_and_display_extreme_biases(\n",
        "    snowflake_luxury_results,\n",
        "    'med_rank_diff\\n(poverty - lux_pov)',\n",
        "    'Luxury vs. Poverty (Snowflake)',\n",
        "    'Poverty-Biased',\n",
        "    'Luxury-Biased'\n",
        ")\n",
        "\n",
        "\n",
        "snowflake_terrorism_results = consolidated_terrorism_bias_df[consolidated_terrorism_bias_df['model_name'].str.contains('Snowflake')]\n",
        "\n",
        "print(\"\\n--- Terrorism vs. Lawfulness Bias Results for Snowflake Model ---\")\n",
        "find_and_display_extreme_biases(\n",
        "    snowflake_terrorism_results,\n",
        "    'med_rank_diff\\n(ter_mon - law_mon)',\n",
        "    'Terrorism vs. Lawfulness (Snowflake)',\n",
        "    'Terrorism-Biased',\n",
        "    'Lawfulness-Biased'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f427a47e"
      },
      "source": [
        "fullname_luxury_results = consolidated_luxury_bias_df[\n",
        "    consolidated_luxury_bias_df['currency_representation_type'] == 'currency_name_dict'\n",
        "]\n",
        "\n",
        "print(\"\\n--- Luxury vs. Poverty Bias Results for Full Name Currency Representation ---\")\n",
        "find_and_display_extreme_biases(\n",
        "    fullname_luxury_results,\n",
        "    'med_rank_diff\\n(poverty - lux_pov)',\n",
        "    'Luxury vs. Poverty (Full Name)',\n",
        "    'Poverty-Biased',\n",
        "    'Luxury-Biased'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowflake_terrorism_results_filtered = consolidated_terrorism_bias_df[\n",
        "    consolidated_terrorism_bias_df['model_name'].str.contains('gte-multilingual-base')\n",
        "]\n",
        "\n",
        "print(\"\\n--- Terrorism vs. Lawfulness Bias Results for Snowflake Model (Filtered) ---\")\n",
        "find_and_display_extreme_biases(\n",
        "    snowflake_terrorism_results_filtered,\n",
        "    'med_rank_diff\\n(ter_mon - law_mon)',\n",
        "    'Terrorism vs. Lawfulness (gte-multilingual-base)',\n",
        "    'Terrorism-Biased',\n",
        "    'Lawfulness-Biased'\n",
        ")"
      ],
      "metadata": {
        "id": "p1w1OgxrP6N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowflake_luxury_results_filtered = consolidated_luxury_bias_df[\n",
        "    consolidated_luxury_bias_df['model_name'].str.contains('gte-multilingual-base')\n",
        "]\n",
        "\n",
        "print(\"\\n--- Luxury vs. Poverty Bias Results for Snowflake Model (Filtered) ---\")\n",
        "find_and_display_extreme_biases(\n",
        "    snowflake_luxury_results_filtered,\n",
        "    'med_rank_diff\\n(poverty - lux_pov)',\n",
        "    'Luxury vs. Poverty (gte-multilingual-base)',\n",
        "    'Poverty-Biased',\n",
        "    'Luxury-Biased'\n",
        ")"
      ],
      "metadata": {
        "id": "ShZ2kH2XQXv6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}